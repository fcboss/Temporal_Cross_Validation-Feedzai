{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json \n",
    "import os\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_units=10, nonlin=F.relu):\n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        self.dense0 = nn.Linear(111, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense1 = nn.Linear(num_units, 10)\n",
    "        self.output = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X.float()))\n",
    "        X = self.dropout(X)\n",
    "        X = F.relu(self.dense1(X))\n",
    "        X = F.softma\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame.rolling -> simple moving average\n",
    "# Weighted moving average sum(w*x) / sum(w)\n",
    "# Exponential moving average\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html\n",
    "\n",
    "def ranking_functions(df):\n",
    "    x = df.metric\n",
    "    average = x.mean()\n",
    "    w = [0,1,2,3,4,5,6,7,8,9]\n",
    "    weighted_avg = sum(w*x) / sum(w)\n",
    "    ewm = x.ewm(span=10).mean().mean()\n",
    "    return pd.Series((average,weighted_avg,ewm),index=['average_','weighted_avg','ewm_'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxmax_param_config(df):\n",
    "    ix_average = df.average_.idxmax()\n",
    "    ix_weighted_avg = df.weighted_avg.idxmax()\n",
    "    ix_ewm = df.ewm_.idxmax()\n",
    "\n",
    "    return pd.Series((df.loc[ix_average].param_config,df.loc[ix_weighted_avg].param_config,df.loc[ix_ewm].param_config)\\\n",
    "                         ,index=['average_','weighted_avg','ewm_'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr(y_true, y_pred):\n",
    "    tnr = recall_score(y_true, y_pred, pos_label = 0) \n",
    "    fpr = 1 - tnr\n",
    "    return fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_s(y_true, y_pred):\n",
    "    tnr = recall_score(y_true, y_pred, pos_label = 1)\n",
    "    return tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(y_true_th,y_proba_th, metric_1, metric_2, min_metric_2= 0.4):\n",
    "    \n",
    "    thresholds = np.arange(0, 1, 0.001)\n",
    "\n",
    "    # evaluate each threshold. We are trying to optimize the first metric, guaranteeing a maximum of 0.4 of the second\n",
    "    metric_1 = recall_s\n",
    "    metric_2 = fpr\n",
    "\n",
    "    scores = np.array([[metric_1(y_true_th, to_labels(y_proba_th, t)) , metric_2(y_true_th, to_labels(y_proba_th, t))]\\\n",
    "              for t in thresholds])\n",
    "    scores_guaranteed_min = scores[:,0]*(scores[:,1]<=min_metric_2)\n",
    "    #print(scores_guaranteed_min)\n",
    "    #print(scores_guaranteed_min)\n",
    "    ix = np.argmax(scores_guaranteed_min)\n",
    "    th = thresholds[ix]\n",
    "    metric = scores_guaranteed_min[ix]\n",
    "    print('Threshold=%.3f, Metric=%.5f' % (th, metric))\n",
    "    \n",
    "    #plt.plot(thresholds, scores[:,0])\n",
    "    #plt.plot(thresholds, scores[:,1])\n",
    "    #plt.show()\n",
    "    \n",
    "    return th,metric\n",
    "\n",
    "#data_test = data.query('date > Start_Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clfs = [NeuralNetClassifier,\n",
    "         RandomForestClassifier,\n",
    "        LogisticRegression,\n",
    "       DecisionTreeClassifier,\n",
    "        lgb.LGBMClassifier,\n",
    "        XGBClassifier,\n",
    "        MLPClassifier,\n",
    "        SVC\n",
    "       ]\n",
    "clfs_names = ['NeuralNetClassifier',\n",
    "         'RandomForestClassifier',\n",
    "        'LogisticRegression',\n",
    "       'DecisionTreeClassifier',\n",
    "        \"lgb_LGBMClassifier\",\n",
    "        'XGBClassifier',\n",
    "        'MLPClassifier',\n",
    "        'SVC'\n",
    "       ]\n",
    "\n",
    "clfs_names_dict = dict(zip(clfs_names,clfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def get_metrics_prod(file_test,model_name_test,clfs_names_dict,y_true):\n",
    "    #get any dict, they are all the same\n",
    "    params_test = pd.read_csv(file_test).get_p.iloc[0]\n",
    "    #Create best model config acording to avg\n",
    "    best_model_config_test = clfs_names_dict[model_name_test](**eval(params_test))\n",
    "    \n",
    "    #train on the whole train\n",
    "    best_model_config_test = best_model_config_test.fit(X_train,y_train)\n",
    "    y_prod_pred = best_model_config_test.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    th,metric = find_threshold(y_true,y_prod_pred, precision_score, accuracy_score, min_metric_2= 0.4)\n",
    "    \n",
    "    print(np.array(y_prod_pred))\n",
    "    print(np.array(y_true))\n",
    "    return th, accuracy_score(y_true,y_prod_pred>th),best_model_config_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_threshold(model_name_test,metric):\n",
    "    #one example\n",
    "    best_model_config_average = best_config_p_metric.loc[model_name_test][metric]\n",
    "    #get any filename to get the params\n",
    "    file_test = 'outputs_models2/'+outputs_metrics.query(f\"model =='{model_name_test}' & param_config == {best_model_config_average}\").filename.iloc[0]\n",
    "    print(file_test)\n",
    "    th,score_test,model = get_metrics_prod(file_test,model_name_test,clfs_names_dict,y_test.values.astype(float))\n",
    "    print(\"threshold test:\",th)\n",
    "    print(\"score test:\",score_test)\n",
    "    return(model_name_test,metric,th,score_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_metrics = pd.read_csv(\"data/outputs_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_</th>\n",
       "      <th>weighted_avg</th>\n",
       "      <th>ewm_</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeuralNetClassifier</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>37</td>\n",
       "      <td>34</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgb_LGBMClassifier</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        average_  weighted_avg  ewm_\n",
       "model                                               \n",
       "DecisionTreeClassifier        37            41    37\n",
       "LogisticRegression            41            41    41\n",
       "MLPClassifier                 19            19    19\n",
       "NeuralNetClassifier            8             8     8\n",
       "RandomForestClassifier        37            34    37\n",
       "SVC                            3             3    28\n",
       "XGBClassifier                 20            20    20\n",
       "lgb_LGBMClassifier            26            26    26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_p_modelconifg = outputs_metrics.groupby(['model','param_config']).apply(ranking_functions)\\\n",
    "    \n",
    "best_config_p_metric =   metrics_p_modelconifg.reset_index()\\\n",
    "    .groupby('model').apply(idxmax_param_config)\n",
    "\n",
    "\n",
    "\n",
    "best_config_p_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/donors_choose_prepared.csv'\n",
    "fold_data = pd.read_csv(file)\n",
    "fold_data['as_of_date'] = pd.to_datetime(fold_data.as_of_date)\n",
    "start_production = fold_data.as_of_date.max() - pd.DateOffset(months=2)\n",
    "start_thresh = fold_data.as_of_date.max() - pd.DateOffset(months=3)\n",
    "start_train = fold_data.as_of_date.max() - pd.DateOffset(months=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = fold_data.query(f'as_of_date > \"{start_train}\" & as_of_date <= \"{start_thresh}\" ')\\\n",
    "            .drop(['entity_id','as_of_date','quickstart_label'],axis=1).applymap(float)\n",
    "y_train = fold_data.query(f'as_of_date > \"{start_train}\" & as_of_date <= \"{start_thresh}\" ')\\\n",
    "        ['quickstart_label'].apply(int)\n",
    "\n",
    "X_test = fold_data.query(f'as_of_date > \"{start_thresh}\" & as_of_date <= \"{start_production}\" ')\\\n",
    "        .drop(['entity_id','as_of_date','quickstart_label'],axis=1).applymap(float)\n",
    "y_test = fold_data.query(f'as_of_date > \"{start_thresh}\" & as_of_date <= \"{start_production}\" ')\\\n",
    "        ['quickstart_label'].apply(int)\n",
    "\n",
    "X_val = fold_data.query(f'as_of_date > \"{start_production}\"  ')\\\n",
    "        .drop(['entity_id','as_of_date','quickstart_label'],axis=1).applymap(float)\n",
    "y_val = fold_data.query(f'as_of_date > \"{start_production}\" ')\\\n",
    "        ['quickstart_label'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_</th>\n",
       "      <th>weighted_avg</th>\n",
       "      <th>ewm_</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeuralNetClassifier</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>37</td>\n",
       "      <td>34</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgb_LGBMClassifier</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        average_  weighted_avg  ewm_\n",
       "model                                               \n",
       "DecisionTreeClassifier        37            41    37\n",
       "LogisticRegression            41            41    41\n",
       "MLPClassifier                 19            19    19\n",
       "NeuralNetClassifier            8             8     8\n",
       "RandomForestClassifier        37            34    37\n",
       "SVC                            3             3    28\n",
       "XGBClassifier                 20            20    20\n",
       "lgb_LGBMClassifier            26            26    26"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_config_p_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "average_\n",
      "outputs_models2/(0, 3, 37)_DecisionTreeClassifier.csv\n",
      "Threshold=0.304, Metric=0.53211\n",
      "[0.25       0.25748503 0.29942418 ... 0.39473684 1.         0.43382353]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "threshold test: 0.304\n",
      "score test: 0.5777317452097359\n",
      "weighted_avg\n",
      "outputs_models2/(0, 3, 41)_DecisionTreeClassifier.csv\n",
      "Threshold=0.313, Metric=0.49694\n",
      "[0.13333333 0.27642276 0.44863014 ... 0.26211454 0.50877193 0.23009624]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "threshold test: 0.313\n",
      "score test: 0.5820818228896945\n",
      "ewm_\n",
      "outputs_models2/(0, 3, 37)_DecisionTreeClassifier.csv\n",
      "Threshold=0.313, Metric=0.49847\n",
      "[0.5        0.29782834 0.34591195 ... 0.26530612 0.35678392 0.43434343]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "threshold test: 0.313\n",
      "score test: 0.5732780942516831\n",
      "LogisticRegression\n",
      "average_\n",
      "outputs_models2/(0, 2, 41)_LogisticRegression.csv\n",
      "Threshold=0.316, Metric=0.53517\n",
      "[0.40513144 0.35021677 0.25730056 ... 0.30044321 0.31186976 0.23265742]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "threshold test: 0.316\n",
      "score test: 0.5803210771620921\n",
      "weighted_avg\n",
      "outputs_models2/(0, 2, 41)_LogisticRegression.csv\n",
      "Threshold=0.316, Metric=0.53517\n",
      "[0.40513144 0.35021677 0.25730056 ... 0.30044321 0.31186976 0.23265742]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "threshold test: 0.316\n",
      "score test: 0.5803210771620921\n",
      "ewm_\n",
      "outputs_models2/(0, 2, 41)_LogisticRegression.csv\n",
      "Threshold=0.316, Metric=0.53517\n",
      "[0.40513144 0.35021677 0.25730056 ... 0.30044321 0.31186976 0.23265742]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "threshold test: 0.316\n",
      "score test: 0.5803210771620921\n",
      "MLPClassifier\n",
      "average_\n",
      "outputs_models2/(0, 6, 19)_MLPClassifier.csv\n",
      "Threshold=0.281, Metric=0.60642\n",
      "[0.39658148 0.09534178 0.28059497 ... 0.28059497 0.40968642 0.27424416]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "threshold test: 0.281\n",
      "score test: 0.6241325737959607\n",
      "weighted_avg\n",
      "outputs_models2/(0, 6, 19)_MLPClassifier.csv\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for model,row in best_config_p_metric.iterrows():\n",
    "    print(model)\n",
    "    for metric_name in row.index:\n",
    "        print(metric_name)\n",
    "        results.append( get_test_threshold(model,metric_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DecisionTreeClassifier', 'average_', 0.307, 0.5650958052822371),\n",
       " ('DecisionTreeClassifier', 'weighted_avg', 0.314, 0.5782496116002072),\n",
       " ('DecisionTreeClassifier', 'ewm_', 0.319, 0.5742102537545314)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
